# Robust Deepfake Detection via Hybrid Convolutional and Transformer Architectures

This project implements a robust deepfake video detection model that integrates a **ResNet3D50** backbone with a **2D Swin Transformer** for spatial encoding and a **Temporal Transformer** for sequence modeling. The hybrid architecture is designed to capture both spatial and temporal inconsistencies introduced in manipulated videos.

## ğŸ§  Architecture Overview

- **ResNet3D50 (Truncated)**: Extracts high-resolution spatio-temporal features from video frames. Final layers removed to preserve `28Ã—28` resolution.
- **Swin Transformer 2D**: Applies localized window-based self-attention to encode detailed spatial features on each video frame.
- **Temporal Transformer**: Models inter-frame dependencies to detect temporal artifacts.
- **Binary Classifier**: Predicts whether a video is `REAL` or `FAKE`.

---

## ğŸ“ Dataset

- **Source**: [Kaggle Deepfake Dataset](https://www.kaggle.com/c/deepfake-detection-challenge/data)
- **Contents**: 1000 training and 400 testing videos, with `metadata.json` indicating labels.
- **Classes**: `REAL` (0), `FAKE` (1)

---

## ğŸ—ï¸ Project Structure

â”œâ”€â”€ main.py # Training and testing pipeline
â”œâ”€â”€ transformer.py # Legacy transformer components (not final)
â”œâ”€â”€ swin_transformer.py # Swin Transformer architecture
â”œâ”€â”€ model_weights.pth # (Optional) Trained weights
â”œâ”€â”€ Balanced Sample/ # Training data directory
â”œâ”€â”€ dfdc_train_part_0/ # Testing data directory
â””â”€â”€ cached_test_samples/ # Optional test set cache



